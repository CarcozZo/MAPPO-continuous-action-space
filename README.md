# MAPPO with Continuous Actions (Tanh Squash)  
MAPPOè¿ç»­åŠ¨ä½œç‰ˆæœ¬ï¼ˆTanhå‹ç¼©ï¼‰

> â­ PyTorch implementation of Multi-Agent Proximal Policy Optimization (MAPPO)  
> Supports **continuous action spaces** with Normal distribution + tanh squash, including log_prob correction.

> â­ åŸºäºPyTorchå®ç°çš„å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆMAPPOï¼‰  
> æ”¯æŒ**è¿ç»­åŠ¨ä½œç©ºé—´**ï¼Œé‡‡ç”¨Normalåˆ†å¸ƒ+Tanhå‹ç¼©ï¼Œå¹¶è¿›è¡Œlog_probä¿®æ­£ã€‚

---

## âœ… Features / ä¸»è¦ç‰¹æ€§

- Continuous actions: Actor outputs mean and log_std  
  è¿ç»­åŠ¨ä½œï¼šActorè¾“å‡ºå‡å€¼meanå’Œå¯¹æ•°æ ‡å‡†å·®log_std  
- Actions squashed into [-1, 1] by tanh  
  åŠ¨ä½œé€šè¿‡tanhå‡½æ•°é™åˆ¶åœ¨[-1,1]èŒƒå›´å†…  
- Correct log_prob correction for PPO loss calculation  
  æ­£ç¡®ä¿®æ­£log_probï¼Œé€‚ç”¨äºPPOæŸå¤±è®¡ç®—  
- Uses a custom environment built on OpenAI Gym  
  åŸºäº OpenAI Gym æ¡†æ¶è‡ªå®šä¹‰ç¯å¢ƒ

---

## ğŸ§  Core Idea / æ ¸å¿ƒåŸç†

- Actor outputs mean and log_std  
  Actorè¾“å‡ºå‡å€¼å’Œå¯¹æ•°æ ‡å‡†å·®  
- Constructs Normal(mean, std) distribution  
  æ„å»ºNormal(mean, std)æ­£æ€åˆ†å¸ƒ  
- Sample raw_action with rsample(), then squash with tanh to [-1,1]  
  ä½¿ç”¨rsample()é‡‡æ ·åŸå§‹åŠ¨ä½œï¼Œå†ç”¨tanhå‹ç¼©åˆ°[-1,1]  
- Correct log_prob to account for tanh transformation:  
  ä¿®æ­£log_probä»¥è€ƒè™‘tanhå˜æ¢ï¼š  

## reward
<img width="518" height="351" alt="image" src="https://github.com/user-attachments/assets/70852486-f402-4259-bf1d-125024263726" />
